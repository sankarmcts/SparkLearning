Here we have a text file with 2 different header lines. Using zipWithIndex function we will
introduce a new autogenerated sequence column at the end

input file has 2 different header lines
auto.csv:
---------
mpg,cylinders,displacement,horsepower,weight,acceleration,year,origin,name
DOUBLE;INT;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;STRING
18,8,307,130,3504,12,70,1,chevrolet chevelle malibu
15,8,350,165,3693,11.5,70,1,buick skylark 320
18,8,318,150,3436,11,70,1,plymouth satellite
16,8,304,150,3433,12,70,1,amc rebel sst

How to exclude (filter) first 2 lines from the rdd?
I mean how to remove the following lines
mpg,cylinders,displacement,horsepower,weight,acceleration,year,origin,name
DOUBLE;INT;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;STRING

// load the file into RDD (c1)
scala> val c1 = sc.textFile("E:\\POCs\\Auto.csv")
c1: org.apache.spark.rdd.RDD[String] = E:\POCs\Auto.csv MapPartitionsRDD[6] at textFile at <console>:24

scala> c1.take(5).foreach(println)
mpg,cylinders,displacement,horsepower,weight,acceleration,year,origin,name
DOUBLE;INT;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;STRING
18,8,307,130,3504,12,70,1,chevrolet chevelle malibu
15,8,350,165,3693,11.5,70,1,buick skylark 320


// Add an autogenerated long column at the end with sequence values
// make a tuple for each lines
scala> val c2 = c1.zipWithIndex()
c2: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[7] at zipWithIndex at <console>:25

// see 0,1,2,3,4.... at the end and now every line is tuple
scala> c2.take(5).foreach(println)
(mpg,cylinders,displacement,horsepower,weight,acceleration,year,origin,name,0)
(DOUBLE;INT;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;DOUBLE;STRING,1)
(18,8,307,130,3504,12,70,1,chevrolet chevelle malibu,2)
(15,8,350,165,3693,11.5,70,1,buick skylark 320,3)
(18,8,318,150,3436,11,70,1,plymouth satellite,4)
(DROPPED,7854545645,6456456456)
scala> D,5755454897,9797979797)

// first line is end with ,0
// second line is end with ,1
// our condition is x._2 > 1
scala> val c3 = c2.filter ( x => x._2 > 1)
c3: org.apache.spark.rdd.RDD[(String, Long)] = MapPartitionsRDD[8] at filter at <console>:25

// here first and second lines are eliminated (header rows removed)
scala> c3.take(5).foreach(println)
(18,8,307,130,3504,12,70,1,chevrolet chevelle malibu,2)
(15,8,350,165,3693,11.5,70,1,buick skylark 320,3)
(18,8,318,150,3436,11,70,1,plymouth satellite,4)
(16,8,304,150,3433,12,70,1,amc rebel sst,5)
(17,8,302,140,3449,10.5,70,1,ford torino,6)

// now remove the last column (0,1,2,3,4) and make String instead of tuple
scala> val c4 = c3.map (x => x._1)
c4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at map at <console>:25

// here the last (auto generated column is removed)
scala> c4.take(5).foreach(println)
18,8,307,130,3504,12,70,1,chevrolet chevelle malibu
15,8,350,165,3693,11.5,70,1,buick skylark 320
18,8,318,150,3436,11,70,1,plymouth satellite
16,8,304,150,3433,12,70,1,amc rebel sst
17,8,302,140,3449,10.5,70,1,ford torino

