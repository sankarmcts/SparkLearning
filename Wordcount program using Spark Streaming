import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.log4j.Logger
import org.apache.log4j.Level

// we are going to reconfigure SparkConfiguration so, we stop existing SparkContext
 sc.stop
 
 Logger.getLogger("org").setLevel(Level.OFF)
 Logger.getLogger("akka").setLevel(Level.OFF)
 
 val conf = new SparkConf().setMaster("local[4]").setAppName("workdcount")
 val ssc = new StreamingContext(conf,Seconds(30))
 val data = ssc.socketTextStream("localhost",7890)
 val wc = data.flatMap(_.split(" ")).map (x => (x,1)).reduceByKey(_+_)
 wc.print()
 wc.saveAsTextFiles("hdfs://localhost:9000/user/streamingexa/ex")
 ssc.start()
 
 
// start netcat and type something there
hadoop@hadoop:/$ nc -lk 7890
i love all beautiful things
god i love all beautiful
god i love
god i




 
 // result is written in hdfs

hadoop@hadoop:/$ hdfs dfs -ls hdfs://localhost:9000/user/streamingexa/
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000
hadoop@hadoop:/$ hdfs dfs -ls hdfs://localhost:9000/user/streamingexa/ex-1549431720000
Found 5 items
-rw-r--r--   3 hadoop supergroup          0 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000/_SUCCESS
-rw-r--r--   3 hadoop supergroup          8 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000/part-00000
-rw-r--r--   3 hadoop supergroup         39 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000/part-00001
-rw-r--r--   3 hadoop supergroup          9 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000/part-00002
-rw-r--r--   3 hadoop supergroup          0 2019-02-06 11:12 hdfs://localhost:9000/user/streamingexa/ex-1549431720000/part-00003
hadoop@hadoop:/$ hdfs dfs -cat hdfs://localhost:9000/user/streamingexa/ex-1549431720000/part-00001
(i,4)
(all,2)
(beautiful,2)
(things,1)
